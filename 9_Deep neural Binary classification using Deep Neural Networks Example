JUPITER NOTEBOOK

pip install tensorflow matplotlib

Load the IMDB Dataset

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Flatten, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt

# Step 1: Load IMDB dataset (only top 10,000 most frequent words)
vocab_size = 10000
maxlen = 500  # max length of review

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)

# Step 2: Pad sequences to ensure consistent input length
X_train = pad_sequences(X_train, maxlen=maxlen)
X_test = pad_sequences(X_test, maxlen=maxlen)

# Step 3: Build the DNN model
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=32, input_length=maxlen),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')  # Binary classification
])

# Step 4: Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Step 5: Train the model
history = model.fit(X_train, y_train, epochs=5, batch_size=512, validation_split=0.2, verbose=1)

# Step 6: Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"\nTest Accuracy: {accuracy:.4f}")

# Step 7: Plot training vs validation accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title("Accuracy over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()





NOTES : 
What this code does:
Loads the IMDB review dataset (binary: positive/negative).
Pads all reviews to the same length.
Uses an embedding layer to represent words as vectors.
Builds a simple DNN with one hidden layer.
Uses sigmoid activation for binary output.
Plots training and validation accuracy.
